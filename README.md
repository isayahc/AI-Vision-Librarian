# Project Name

[![Build Status](https://img.shields.io/travis/username/reponame.svg?style=flat-square)](https://travis-ci.org/username/reponame)
[![Coverage Status](https://img.shields.io/coveralls/username/reponame.svg?style=flat-square)](https://coveralls.io/github/username/reponame)
[![License](https://img.shields.io/github/license/isayahc/python-sample-template.svg?style=flat-square)](LICENSE)

## Description

Brief description of your project.

## Problem Statement

there is a large body of images / videos that have been clustered via tags. Usually generated by a human. Due to this a lot of information is lost. Making operations such as search via tags leaving users of image related services unhappy. It ahs also lead to video streaming sites having a plethora of data that has never been seen.

Even with solutions that are able to segment and identify the images are limited as they need to be fine tuned in order to be verbose. Such as identify objects in relation to other information with fine tuning for a given dataset. This within itself is not scalable to new information.

## Solution

Using a multimodal Large Language Model on images and videos can be used  to generate a set of tags. These tags can be extremely specific for users to find exactly what they want. In addition it is easier to search bodies of text over bodies of images. These tags can point to the image.

## Features

- Feature 1
- Feature 2
- Feature 3

## Installation

\```bash
# commands to install the project
\```

## Usage

\```bash
# commands or instructions for using your project
\```

## Contributing

Instructions for how to contribute to the project.

## License

[MIT](LICENSE)
